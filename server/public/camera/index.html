<head>

<script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>    

<script src="https://download.affectiva.com/js/3.2/affdex.js"></script>    

    <link rel="stylesheet" href="demo/css/bootstrap-darkly.min.css" />
    <link href="demo/css/demo.css" rel="stylesheet" type="text/css" />

    <!-- outside vendor files -->
    <script src="demo/js/vendor/jquery-2.1.4.js"></script>
    <script src="demo/js/vendor/d3.min.js"></script>

    <!-- demo files -->
    <script src="demo/js/run_demo.js"></script>
    <script src="https://download.affectiva.com/js/3.2/affdex.js"></script>
    

</head><body>


    <div id="text-width"></div>
<div id="lightbox"></div>
<div id="demo-setup">
    <div class="row">
        <div class="col-md-3 text-center">
            <div id="facevideo-node"></div>
        </div>
    </div>
    <div id="messages" class="row">
        <div class="col-md-6 col-md-offset-3 demo-message" id="msg-starting-webcam">
            <div class="alert alert-info" role="alert">Connecting to webcam...</div>
        </div>
        <div class="col-md-6 col-md-offset-3 demo-message" id="msg-webcam-failure">
            <div class="alert alert-danger" role="alert"><strong>Error: </strong>Failed to connect to webcam.</div>
        </div>
        <div class="col-md-6 col-md-offset-3 demo-message" id="msg-detector-status">
            <div class="alert alert-info" role="alert">Loading and starting the emotions detector, this may take few minutes ...</div>
        </div>
        <div class="col-md-6 col-md-offset-3 demo-message" id="msg-bad-url">
            <div class="alert alert-warning" role="alert"><strong>Error: </strong>Please enter a valid YouTube URL or search phrase.</div>
        </div>
        <div class="col-md-6 col-md-offset-3 demo-message" id="msg-short-video">
            <div class="alert alert-warning" role="alert"><strong>Error: </strong>That YouTube video is too short.</div>
        </div>
        <div class="col-md-8 col-md-offset-2" id="instructions">
            <div class="well">
                <p><strong>Welcome!</strong></p>
                <p>This demo uses Affectiva's JavaScript SDK to analyze your emotions as you watch a YouTube video. Before starting this demo, please make sure that your face is centered in the screen and the room lighting is adequate.</p>
                <p><strong>Instructions:</strong></p>
                <p>Search a YouTube video by keyword or enter its URL, and with your webcam turned on, you'll be able to see your emotions both during the video and during playback:</p>
                
                <div class="form-group">
                    <div class="input-group">
                        <input type="text" class="form-control" id="start-form" placeholder="Enter a URL or search YouTube here..." autofocus>
                        <span class="input-group-btn">
                            <button id="btn-start" class="btn btn-primary" type="button">Start</button>
                        </span>
                    </div>
                </div>
            </div>
            
            <div class="list-group row" id="search-results"></div>
            
            <div class="well">
                <p>Here are some selected videos to get you started:</p>
                <div class="row">
                    <div class="col-md-4 starter-container" id="example-0"></div>
                    <div class="col-md-4 starter-container" id="example-1"></div>
                    <div class="col-md-4 starter-container" id="example-2"></div>
                </div>
                <div class="row">
                    <div class="col-md-4 starter-container" id="example-3"></div>
                    <div class="col-md-4 starter-container" id="example-4"></div>
                    <div class="col-md-4 starter-container" id="example-5"></div>
                </div>
            </div>
            
        </div>
    </div>
    
</div>

<div class="row" id="video-container">
    <div class="col-md-12 text-center">
        <div id="player" style="pointer-events: none"></div>
        <div id="svg-container">
            <div class="row">
                <div id="ul-wrapper" class="col-md-2">
                    <ul id="nav">
                    <li class="all buttons all-face all-box box-squared" id="all"><span>Show all</span></li>
                    <li class="joy buttons smiling-face joy-box box-squared" id="joy" onClick="JSSDKDemo.responses(this.id)"><span>Joy</span></li>
                    <li class="anger buttons angry-face anger-box box-squared" id="anger" onClick="JSSDKDemo.responses(this.id)"><span>Anger</span></li>
                    <li class="disgust buttons disgusted-face disgust-box box-squared" id="disgust"  onClick="JSSDKDemo.responses(this.id)"><span>Disgust</span></li>
                    <li class="contempt buttons contempt-face contempt-box box-squared" id="contempt"  onClick="JSSDKDemo.responses(this.id)"><span>Contempt</span></li>
                    <li class="surprise buttons surprise-face surprise-box box-squared" id="surprise"  onClick="JSSDKDemo.responses(this.id)"><span>Surprise</span></li>
                    </ul>
                </div>
                <div id="svg-wrapper" class="col-md-8 text-center">
                    <svg id="svg-curve"></svg>
                </div>

                <div id="play-again" class="col-md-2">
                  <div class="well">
                    <p>Frame capture complete!</p>
                    <p><small>Watch the video synchronized with your emotions.</small></p>
                    <p>&nbsp;</p>
                    <button id="btn-play-again" class="btn btn-primary">Ok, got it</button>
                  </div>
                </div>
            </div>
        </div>
    </div>
</div>


  <div class="container-fluid">
    <div class="row">
      <div class="col-md-8" id="affdex_elements" style="width:70px;height:70px;"></div>
      <div class="col-md-4">
        <div style="height:25em;">
          <strong>EMOTION TRACKING RESULTS</strong>
          <div id="results" style="word-wrap:break-word;"></div>
        </div>
        <div>
          <strong>DETECTOR LOG MSGS</strong>
        </div>
        <div id="logs"></div>
      </div>
    </div>
    <div>
      <button id="start" onclick="onStart()">Start</button>
      <button id="stop" onclick="onStop()">Stop</button>
      <button id="reset" onclick="onReset()">Reset</button>
      <h3>Affectiva JS SDK CameraDetector to track different emotions.</h3>
      <p>
        <strong>Instructions</strong>
        </br>
        Press the start button to start the detector.
        <br/> When a face is detected, the probabilities of the different emotions are written to the DOM.
        <br/> Press the stop button to end the detector.
      </p>
    </div>
  </div>
  <script>
            // SDK Needs to create video and canvas nodes in the DOM in order to function
      // Here we are adding those nodes a predefined div.
      var divRoot = $("#affdex_elements")[0];
      var width = 70;
      var height = 70;
      var faceMode = affdex.FaceDetectorMode.LARGE_FACES;
      //Construct a CameraDetector and specify the image width / height and face detector mode.
      var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);

      //Enable detection of all Expressions, Emotions and Emojis classifiers.
      detector.detectAllEmotions();
      detector.detectAllExpressions();
      detector.detectAllEmojis();
      detector.detectAllAppearance();

      //Add a callback to notify when the detector is initialized and ready for runing.
      detector.addEventListener("onInitializeSuccess", function() {
        log('#logs', "The detector reports initialized");
        //Display canvas instead of video feed because we want to draw the feature points on it
        $("#face_video_canvas").css("display", "block");
        $("#face_video").css("display", "none");
      });

      function log(node_name, msg) {
        $(node_name).append("<span>" + msg + "</span><br />")
      }

      //function executes when Start button is pushed.
      function onStart() {
        if (detector && !detector.isRunning) {
          $("#logs").html("");
          detector.start();
        }
        log('#logs', "Clicked the start button");
      }

      //function executes when the Stop button is pushed.
      function onStop() {
        log('#logs', "Clicked the stop button");
        if (detector && detector.isRunning) {
          detector.removeEventListener();
          detector.stop();
        }
      };

      //function executes when the Reset button is pushed.
      function onReset() {
        log('#logs', "Clicked the reset button");
        if (detector && detector.isRunning) {
          detector.reset();

          $('#results').html("");
        }
      };

      //Add a callback to notify when camera access is allowed
      detector.addEventListener("onWebcamConnectSuccess", function() {
        log('#logs', "Webcam access allowed");
      });

      //Add a callback to notify when camera access is denied
      detector.addEventListener("onWebcamConnectFailure", function() {
        log('#logs', "webcam denied");
        console.log("Webcam access denied");
      });

      //Add a callback to notify when detector is stopped
      detector.addEventListener("onStopSuccess", function() {
        log('#logs', "The detector reports stopped");
        $("#results").html("");
      });

      //Add a callback to receive the results from processing an image.
      //The faces object contains the list of the faces detected in an image.
      //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
      detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
          console.log(faces[0].emotions);
        $('#results').html("");
        log('#results', "Timestamp: " + timestamp.toFixed(2));
        log('#results', "Number of faces found: " + faces.length);
        if (faces.length > 0) {
          log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
          log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
          drawFeaturePoints(image, faces[0].featurePoints);
        }
      });

      //Draw the detected facial feature points on the image
      function drawFeaturePoints(img, featurePoints) {
        var contxt = $('#face_video_canvas')[0].getContext('2d');

        var hRatio = contxt.canvas.width / img.width;
        var vRatio = contxt.canvas.height / img.height;
        var ratio = Math.min(hRatio, vRatio);

        contxt.strokeStyle = "#FFFFFF";
        for (var id in featurePoints) {
          contxt.beginPath();
          contxt.arc(featurePoints[id].x,
            featurePoints[id].y, 2, 0, 2 * Math.PI);
          contxt.stroke();

        }
      }

</script>
</body>
